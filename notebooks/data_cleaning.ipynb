{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qc8TO52OZT3u",
    "outputId": "5a292cc5-35a3-4e68-a1a0-86797b532ed6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading all article files...\n",
      "üîπ Reading from: /home/data/raw/train/news.tsv\n",
      "  ‚úîÔ∏è Loaded 38189 rows.\n",
      "üîπ Reading from: /home/data/raw/dev/news.tsv\n",
      "  ‚úîÔ∏è Loaded 67583 rows.\n",
      "üîπ Reading from: /home/data/raw/test/news.tsv\n",
      "  ‚úîÔ∏è Loaded 92132 rows.\n",
      "üßπ Cleaning abstracts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:00<00:00, 28016.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total unique articles: (17, 8)\n",
      "üì• Loading all behavior logs...\n",
      "üîπ Reading from: /home/data/raw/train/behaviors.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úîÔ∏è Loaded 47954 rows.\n",
      "üîπ Reading from: /home/data/raw/dev/behaviors.tsv\n",
      "  ‚úîÔ∏è Loaded 1747 rows.\n",
      "üîπ Reading from: /home/data/raw/test/behaviors.tsv\n",
      "  ‚úîÔ∏è Loaded 110667 rows.\n",
      "üìä Total behavior logs: (160368, 5)\n",
      "üîÑ Parsing impressions to extract clicked and all articles (fully vectorized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing impressions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 160368/160368 [00:03<00:00, 48865.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned articles and behavior logs saved to: /home/data/processed\n"
     ]
    }
   ],
   "source": [
    "# 01_data_cleaning.ipynb - Colab-Compatible Version (No Drive, Local Runtime)\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# === PATH SETUP FOR LOCAL COLAB RUNTIME ===\n",
    "RAW_DIRS = [\n",
    "    \"/home/data/raw/train\",\n",
    "    \"/home/data/raw/dev\",\n",
    "    \"/home/data/raw/test\"\n",
    "]\n",
    "PROCESSED_DIR = \"/home/data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# === HELPER: CLEAN TEXT ===\n",
    "def clean_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    if str(text).startswith(\"http\"):\n",
    "        return \"\"  # skip URLs mistakenly passed\n",
    "    text = BeautifulSoup(str(text), \"html.parser\").get_text()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# === LOAD AND COMBINE ARTICLES ===\n",
    "print(\"üì• Loading all article files...\")\n",
    "all_articles = []\n",
    "for dir_path in RAW_DIRS:\n",
    "    path = os.path.join(dir_path, \"news.tsv\")\n",
    "    print(f\"üîπ Reading from: {path}\")\n",
    "    df = pd.read_csv(path, sep='\\t', header=None,\n",
    "                     names=[\"id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"entity\"],\n",
    "                     on_bad_lines='skip', encoding='utf-8')\n",
    "    print(f\"  ‚úîÔ∏è Loaded {df.shape[0]} rows.\")\n",
    "    all_articles.append(df)\n",
    "articles_df = pd.concat(all_articles).drop_duplicates(subset='id').reset_index(drop=True)\n",
    "print(\"üßπ Cleaning abstracts...\")\n",
    "articles_df['abstract_clean'] = articles_df['abstract'].progress_apply(clean_text)\n",
    "print(\"üìä Total unique articles:\", articles_df.shape)\n",
    "\n",
    "# === LOAD AND COMBINE BEHAVIORS ===\n",
    "print(\"üì• Loading all behavior logs...\")\n",
    "all_behaviors = []\n",
    "for dir_path in RAW_DIRS:\n",
    "    path = os.path.join(dir_path, \"behaviors.tsv\")\n",
    "    print(f\"üîπ Reading from: {path}\")\n",
    "    df = pd.read_csv(path, sep='\\t', header=None,\n",
    "                     names=[\"impression_id\", \"user_id\", \"timestamp\", \"history\", \"impressions\"],\n",
    "                     on_bad_lines='skip', encoding='utf-8')\n",
    "    print(f\"  ‚úîÔ∏è Loaded {df.shape[0]} rows.\")\n",
    "    all_behaviors.append(df)\n",
    "behaviors_df = pd.concat(all_behaviors).reset_index(drop=True)\n",
    "print(\"üìä Total behavior logs:\", behaviors_df.shape)\n",
    "\n",
    "# === PARSE CLICKS (Fully Vectorized) ===\n",
    "print(\"üîÑ Parsing impressions to extract clicked and all articles (fully vectorized)...\")\n",
    "\n",
    "clicked_articles = []\n",
    "all_articles_list = []\n",
    "\n",
    "for impression in tqdm(behaviors_df['impressions'], desc=\"Parsing impressions\"):\n",
    "    items = str(impression).split()\n",
    "    clicked = [i.split('-')[0] for i in items if i.endswith('-1')]\n",
    "    all_ids = [i.split('-')[0] for i in items]\n",
    "    clicked_articles.append(clicked)\n",
    "    all_articles_list.append(all_ids)\n",
    "\n",
    "behaviors_df['clicked_articles'] = clicked_articles\n",
    "behaviors_df['all_articles'] = all_articles_list\n",
    "\n",
    "# === EXPORT CLEANED DATA ===\n",
    "articles_df.to_csv(os.path.join(PROCESSED_DIR, \"cleaned_articles.csv\"), index=False)\n",
    "behaviors_df.to_csv(os.path.join(PROCESSED_DIR, \"cleaned_behaviors.csv\"), index=False)\n",
    "\n",
    "print(\"‚úÖ Cleaned articles and behavior logs saved to:\", PROCESSED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0VafWgjA_Ua",
    "outputId": "af1073ad-4249-408d-c82f-8870f0d5949d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-007a46964b88>:19: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  behaviors_df = pd.read_csv(os.path.join(PROCESSED_DIR, \"cleaned_behaviors.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded data: (17, 8) (160368, 7)\n",
      "‚úÖ Tokenized articles and behavior logs saved to: /home/data/processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up directories (if not already set)\n",
    "RAW_DIRS = [\n",
    "    \"/home/data/raw/train\",\n",
    "    \"/home/data/raw/dev\",\n",
    "    \"/home/data/raw/test\"\n",
    "]\n",
    "PROCESSED_DIR = \"/home/data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# === Load Cleaned Data ===\n",
    "# Load the cleaned articles and behaviors\n",
    "articles_df = pd.read_csv(os.path.join(PROCESSED_DIR, \"cleaned_articles.csv\"))\n",
    "behaviors_df = pd.read_csv(os.path.join(PROCESSED_DIR, \"cleaned_behaviors.csv\"))\n",
    "\n",
    "print(\"üìä Loaded data:\", articles_df.shape, behaviors_df.shape)\n",
    "\n",
    "# === Tokenizer Setup ===\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Using BERT base uncased tokenizer\n",
    "\n",
    "# Tokenization function with NaN handling\n",
    "def tokenize_article(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return []  # Return empty list for NaN or empty strings\n",
    "    return tokenizer.encode(text, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# Apply tokenization to the article abstracts\n",
    "articles_df['tokenized_abstract'] = articles_df['abstract_clean'].apply(lambda x: tokenize_article(str(x)))\n",
    "\n",
    "# === Map User Histories to Tokenized Articles ===\n",
    "# Create a mapping of article ID to tokenized article\n",
    "article_mapping = dict(zip(articles_df['id'], articles_df['tokenized_abstract']))\n",
    "\n",
    "# Function to map user article history to tokenized articles\n",
    "def get_article_tokens_for_user(article_ids):\n",
    "    return [article_mapping.get(article_id, []) for article_id in article_ids]\n",
    "\n",
    "# Apply the mapping for clicked and all articles\n",
    "behaviors_df['clicked_articles_tokens'] = behaviors_df['clicked_articles'].apply(lambda x: get_article_tokens_for_user(eval(x)))\n",
    "behaviors_df['all_articles_tokens'] = behaviors_df['all_articles'].apply(lambda x: get_article_tokens_for_user(eval(x)))\n",
    "\n",
    "# === Save Processed Data ===\n",
    "# Save the processed articles and behaviors\n",
    "articles_df.to_csv(os.path.join(PROCESSED_DIR, \"tokenized_articles.csv\"), index=False)\n",
    "behaviors_df.to_csv(os.path.join(PROCESSED_DIR, \"processed_behaviors_with_tokens.csv\"), index=False)\n",
    "\n",
    "print(\"‚úÖ Tokenized articles and behavior logs saved to:\", PROCESSED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CINmuJwtEIHq",
    "outputId": "0acfaec2-6557-4b78-e09c-83345cae49ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete. Tokenized articles saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-e38f71032339>:23: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cleaned_behaviours_df = pd.read_csv('/home/data/processed/cleaned_behaviors.csv')  # Replace with actual path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned User Histories Sample:\n",
      "   user_id                  clicked_articles  \\\n",
      "0   U87243  [N94157, N78699, N71090, N31174]   \n",
      "1  U598644                  [N25587, N36266]   \n",
      "2  U532401                          [N47925]   \n",
      "3  U593596                         [N114935]   \n",
      "4  U239687                          [N86258]   \n",
      "\n",
      "                                             history  \n",
      "0  [N8668, N39081, N65259, N79529, N73408, N43615...  \n",
      "1  [N56056, N8726, N70353, N67998, N83823, N11110...  \n",
      "2  [N128643, N87446, N122948, N9375, N82348, N129...  \n",
      "3  [N31043, N39592, N4104, N8223, N114581, N92747...  \n",
      "4  [N65250, N122359, N71723, N53796, N41663, N414...  \n",
      "‚úÖ User Histories Sample:\n",
      "   user_id                  clicked_articles  \\\n",
      "0   U87243  [N94157, N78699, N71090, N31174]   \n",
      "1  U598644                  [N25587, N36266]   \n",
      "2  U532401                          [N47925]   \n",
      "3  U593596                         [N114935]   \n",
      "4  U239687                          [N86258]   \n",
      "\n",
      "                                             history  \n",
      "0  [N8668, N39081, N65259, N79529, N73408, N43615...  \n",
      "1  [N56056, N8726, N70353, N67998, N83823, N11110...  \n",
      "2  [N128643, N87446, N122948, N9375, N82348, N129...  \n",
      "3  [N31043, N39592, N4104, N8223, N114581, N92747...  \n",
      "4  [N65250, N122359, N71723, N53796, N41663, N414...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Step 1: Load and clean the articles DataFrame (replace the path with the actual file path)\n",
    "cleaned_articles_df = pd.read_csv('/home/data/processed/cleaned_articles.csv')  # Replace with actual path\n",
    "\n",
    "# Check for NaN values in the 'abstract_clean' column and replace them with an empty string\n",
    "cleaned_articles_df['abstract_clean'] = cleaned_articles_df['abstract_clean'].fillna('')\n",
    "\n",
    "# Tokenize the 'abstract_clean' column\n",
    "def tokenize_article(text):\n",
    "    return text.split()\n",
    "\n",
    "# Apply tokenization to the cleaned articles\n",
    "cleaned_articles_df['tokenized_abstract'] = cleaned_articles_df['abstract_clean'].apply(tokenize_article)\n",
    "\n",
    "# Save the tokenized articles to CSV (replace with actual path)\n",
    "cleaned_articles_df.to_csv('/home/data/processed/tokenized_articles.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Tokenization complete. Tokenized articles saved.\")\n",
    "\n",
    "# Step 2: Load and clean the user behavior DataFrame\n",
    "cleaned_behaviours_df = pd.read_csv('/home/data/processed/cleaned_behaviors.csv')  # Replace with actual path\n",
    "\n",
    "# Step 3: Clean and convert clicked_articles and history into lists\n",
    "# Convert 'clicked_articles' from string representation of a list into an actual list\n",
    "cleaned_behaviours_df['clicked_articles'] = cleaned_behaviours_df['clicked_articles'].apply(ast.literal_eval)\n",
    "\n",
    "# Handle NaN values in 'history' column and convert to list\n",
    "cleaned_behaviours_df['history'] = cleaned_behaviours_df['history'].fillna('')\n",
    "\n",
    "# Convert 'history' from space-separated string of article IDs into a list\n",
    "cleaned_behaviours_df['history'] = cleaned_behaviours_df['history'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "\n",
    "# Preview the cleaned user histories\n",
    "print(\"‚úÖ Cleaned User Histories Sample:\")\n",
    "print(cleaned_behaviours_df[['user_id', 'clicked_articles', 'history']].head())\n",
    "\n",
    "# Step 4: Extract User Histories and create a sample\n",
    "user_histories = cleaned_behaviours_df[['user_id', 'clicked_articles', 'history']]\n",
    "\n",
    "print(\"‚úÖ User Histories Sample:\")\n",
    "print(user_histories.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbd9kRi4F46t",
    "outputId": "8227d470-f1da-4017-8baf-82c4532204c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete using regex. Tokenized articles saved.\n",
      "‚úÖ Cleaned User Histories Sample:\n",
      "   user_id                  clicked_articles  \\\n",
      "0   U87243  [N94157, N78699, N71090, N31174]   \n",
      "1  U598644                  [N25587, N36266]   \n",
      "2  U532401                          [N47925]   \n",
      "3  U593596                         [N114935]   \n",
      "4  U239687                          [N86258]   \n",
      "\n",
      "                                             history  \n",
      "0  [N8668, N39081, N65259, N79529, N73408, N43615...  \n",
      "1  [N56056, N8726, N70353, N67998, N83823, N11110...  \n",
      "2  [N128643, N87446, N122948, N9375, N82348, N129...  \n",
      "3  [N31043, N39592, N4104, N8223, N114581, N92747...  \n",
      "4  [N65250, N122359, N71723, N53796, N41663, N414...  \n",
      "‚úÖ Cleaned user histories saved to: /home/data/processed/cleaned_user_histories.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "articles_path = '/home/data/processed/cleaned_articles.csv'\n",
    "behaviors_path = '/home/data/processed/cleaned_behaviors.csv'\n",
    "output_path = '/home/data/processed/cleaned_user_histories.csv'\n",
    "\n",
    "# --- Safe tokenizer (no nltk) ---\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Load cleaned articles\n",
    "cleaned_articles_df = pd.read_csv(articles_path)\n",
    "\n",
    "if 'abstract_clean' in cleaned_articles_df.columns:\n",
    "    cleaned_articles_df['abstract_clean'] = cleaned_articles_df['abstract_clean'].fillna('').astype(str)\n",
    "    cleaned_articles_df['abstract_tokens'] = cleaned_articles_df['abstract_clean'].apply(simple_tokenizer)\n",
    "    print(\"‚úÖ Tokenization complete using regex. Tokenized articles saved.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è 'abstract_clean' column not found in article data.\")\n",
    "\n",
    "# Load cleaned behaviors\n",
    "cleaned_behaviours_df = pd.read_csv(behaviors_path, low_memory=False)\n",
    "\n",
    "# Convert strings to lists\n",
    "if 'clicked_articles' in cleaned_behaviours_df.columns:\n",
    "    cleaned_behaviours_df['clicked_articles'] = cleaned_behaviours_df['clicked_articles'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "    )\n",
    "\n",
    "if 'history' in cleaned_behaviours_df.columns:\n",
    "    cleaned_behaviours_df['history'] = cleaned_behaviours_df['history'].fillna('').astype(str)\n",
    "    cleaned_behaviours_df['history'] = cleaned_behaviours_df['history'].apply(lambda x: x.split())\n",
    "\n",
    "# Preview\n",
    "print(\"‚úÖ Cleaned User Histories Sample:\")\n",
    "print(cleaned_behaviours_df[['user_id', 'clicked_articles', 'history']].head())\n",
    "\n",
    "# Save output\n",
    "cleaned_behaviours_df[['user_id', 'clicked_articles', 'history']].to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Cleaned user histories saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "HWh98BAjJILR",
    "outputId": "941b51d2-2130-43e6-cc55-1e781e7c55a2"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_b2cf1ab3-4849-420f-bfe9-d4ba84f02e08\", \"cleaned_behaviors.csv\", 163357336)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('/home/data/processed/cleaned_behaviors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxThTehnYmbU",
    "outputId": "296b3788-4177-46b0-92b4-401d85de425b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install transformers evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385,
     "referenced_widgets": [
      "71357f621b87445683fc36336af64669",
      "6c95b7dd56b74ccb9fbd1ea015f94016",
      "2a5fb114f6944c97b18e0490c1c6980d",
      "4b365533bf4242b8aec0f3e59c1858c0",
      "fdf765f7565a4355838311559b897723",
      "1d381c05ca1c431a99a2dfbb2b213a86",
      "e708a1de528a41c99f057c3b2ead340e",
      "98860e818cf34bdabfe0eacca60f69e4",
      "d43a8c2d19614d90a343a5dd2af733de",
      "3546773424a14f0a8df93626615bf6f1",
      "b1872b753cb4486cb3ab1921176ec27b",
      "b7ee65d02a9e4040aaf4dbd3bac86bdc",
      "cc1ccc48b3c5482a966416319a60ef15",
      "800008c1d48243df8e4096ea6a2f2022",
      "f3dfec99026246c0acb2af94a580179f",
      "b72ffa3db87746e795c4bcd0251da70d",
      "f9740b5c109d4044b1d1c576d14ecaf3",
      "3f862d846b4b4683b58b3aa113df098e",
      "4bba112737ea430ea2c58124f026231d",
      "07ab39fdcff04b7bb53e7a3fb9ce22ef",
      "369794c1b61c4b1da6155f4efa44f438",
      "ed290d6de74041f88bd1e5b2ba1f2d4f",
      "98cd4d3cd8b14d91a1c51962c1e9624e",
      "60fbfbd549764b8f96d7311bf1b1a619",
      "f5474faa0f5e489a9100b7485f1314db",
      "f881b26a9e484e859e78157290c57671",
      "b90014eb873142918dee5b1ecef6d09a",
      "691891758fdb497e91369900be329d39",
      "5a4dcfe6a47e475690103ab0aa1f7af7",
      "2e925d5968de49908148c080e51457f0",
      "c42c8b22917748d09421fadcbeb24564",
      "0749d978e7e14dc4a0e62eaddcc90d65",
      "3e423234fcb646bd9b283499f3b841dc",
      "1861d7ec9059463499d1fe89ba87ed8d",
      "8d1b010ce5d240f2af01a33a5348cd6e",
      "4bce8cbfc3b14851a7c3c24f779a857d",
      "f1c789c2f9e14caa9adfadf5d2df912e",
      "44daf150fa25443e8471caae2eec0425",
      "b2048400cfe943c89562b3cdf5d776ee",
      "6d6488339a5e4f19baa684e9bbbbdbca",
      "d9ceb9272dc243ae8c0e21b7075fa0e4",
      "83169281af8848318b7fbd6dd6237b19",
      "db21f6ffbf1241b5b61bc0834718591b",
      "a4ea18540edc4b93bf75bbec4acfa95e",
      "295b12eafae944b998577b2f7289f8b6",
      "baf6e3045c4e4b76bb48d7f654166146",
      "fe069259b22945748df2795505bd142a",
      "580fd8fcffc241e1b4765997a941f5c8",
      "7c389ec39f5140a6904ca84963ea8b6c",
      "c6a5f684409b4038a3deb72e4c0657aa",
      "4cda704a96c045a0b0ee6d4adad456ee",
      "71d8fb68117543ee9d016e87c988cdf3",
      "eed83c2d5d754170875634f61257fff1",
      "e61fda1c02d54d0490f5026ba0463e4c",
      "8f49d13e7cd34006a776c7d9257efe53",
      "fe846737a74d43d484fb6000f3228e45",
      "5f7cf78e3d30474c856a0511f96f810e",
      "82b092bb45b84eb282f9284b5f17bca1",
      "05561e7440bd48a7bd1bca0f93eb5bf6",
      "c5a7a9e8bb2b44e6b623c151d2fa640c",
      "183c662af84f4a879bba6958cd97bb3d",
      "5304860a0ab648d9a0b53c355c24cafe",
      "36a55f1d858f45d9ae49bc6cb1b28096",
      "21688acbd756425e95113e27cdff46f4",
      "a53f3324835146b49861c1bd40e82614",
      "b9bcdbba2a3c4f1f9b9b080d3d0b6357"
     ]
    },
    "id": "sDzF0YaLXuxP",
    "outputId": "7173881c-88bb-4d0a-f258-e4dcb274ec28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71357f621b87445683fc36336af64669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ee65d02a9e4040aaf4dbd3bac86bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cd4d3cd8b14d91a1c51962c1e9624e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1861d7ec9059463499d1fe89ba87ed8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295b12eafae944b998577b2f7289f8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe846737a74d43d484fb6000f3228e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Generating summaries using BART...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:00<00:00, 63550.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All summaries saved to summarized_articles_bart.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the cleaned articles CSV (make sure this file is uploaded)\n",
    "cleaned_articles_df = pd.read_csv('/content/sample_data/processed/cleaned_articles.csv')\n",
    "\n",
    "# Load BART tokenizer and model\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define summarization function\n",
    "def generate_summary(text, model, tokenizer, max_input_length=1024, max_output_length=100):\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_output_length,\n",
    "        min_length=25,\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Apply summarization to each article's abstract_clean\n",
    "summaries = []\n",
    "print(\"‚öôÔ∏è Generating summaries using BART...\")\n",
    "for text in tqdm(cleaned_articles_df['abstract_clean']):\n",
    "    summary = generate_summary(text, model, tokenizer)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Add new column to DataFrame\n",
    "cleaned_articles_df['summary'] = summaries\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_path = 'summarized_articles_bart.csv'\n",
    "cleaned_articles_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ All summaries saved to {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjTdYzUOC4j1",
    "outputId": "e67667c8-d08d-4d78-c05e-f444419f4899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'category', 'subcategory', 'title', 'abstract', 'url', 'entity',\n",
      "       'abstract_clean'],\n",
      "      dtype='object')\n",
      "                                        abstract  abstract_clean\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html             NaN\n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html             NaN\n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html             NaN\n",
      "3  https://assets.msn.com/labs/mind/AAJ4lap.html             NaN\n",
      "4  https://assets.msn.com/labs/mind/AAJwoxD.html             NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/sample_data/processed/cleaned_articles.csv\")\n",
    "print(df.columns)\n",
    "print(df[['abstract', 'abstract_clean']].head())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
